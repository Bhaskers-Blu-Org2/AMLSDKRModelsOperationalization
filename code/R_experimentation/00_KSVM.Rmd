---
title: "Basic modeling using kernel SVM in R"
author: "George Iordanescu"
affiliation: Microsoft
date: "`r format(Sys.time(), '%B %d %Y')`"
output:
  html_document:
    df_print: paged
tags:
- ML
- SVM
- Kernel
- R
abstract: |
  Basic modeling using kernel SVM in R.
  USes Simulated data. Does not do hyperparameter tuning.
---

Experiment params 

```{r}
n_samples = 5e3
disk_radius = 0.7

set.seed(33)
```

```{r}
# all data
data_x = cbind(2*(runif(n_samples))-1,2*runif(n_samples)-1)
data_y = as.numeric((data_x[,2]^2 + data_x[,1]^2)<disk_radius)
```
 
We can add a probability to make separation fuzzy

 - class_separability is the the steepness of the sigmoid curve, the larger it is, the better class separability is.  
  -__class_separability = 2e0__ does some mixture. it is a good value to put svm to  work hard as you can see form the large set of support vectors (SV). This shows the model is overfitting, and cross-validation hyper-param tuning is needed.  
  -__class_separability = 2e1__ does good class separability, similar to binary separation we get by using "<disk_radius" above. The SVs list in now "clean" and focused on the border.  
   
 - if we use probability instead of hard separation, we should make data_y continuous (i.e use "-disk_radius" as opposed to "<disk_radius"), before passing it through the sigmoid   
```{r}

class_separability = 2e1
prob = 1/(1+exp(-class_separability*((data_x[,2]^2 + data_x[,1]^2)-disk_radius)))
data_y = 2*rbinom(n_samples,1,prob)-1
```

train-test split
```{r} 
 ## use 80% of the samples fpr training
train_size = floor(0.80 * nrow(data_x))

## set the seed to make your partition reproducible

train_ind = sample(seq_len(nrow(data_x)), size = train_size)

train_x = data_x[train_ind, ]
train_y = data_y[train_ind]

test_x = data_x[-train_ind, ]
test_y = data_y[-train_ind]
```

Plot some data
```{r}
train_and_test_data_plot = function() {
  plot(train_x,col=ifelse(train_y=="1",1,3),pch=19,main="Train and test data",asp=1)
  points(test_x,col=ifelse(test_y=="1",2,4),pch=19)
  #par(new=TRUE)
  #plot(test_x,col=ifelse(test_y=="1",2,4),pch=19)
  legend("topright",c("train_Y=1","train_Y=-1", "test_Y=1","test_Y=-1"),pch=19,col=c(1,3,2,4),inset=0.05,bg=gray(1),cex=1.5)
}

#png(file="train_and_test_data.png",width=400,height=400)
train_and_test_data_plot()
#dev.off()



```
Do some basic model selection:
https://cran.r-project.org/web/packages/kernlab/kernlab.pdf
"empirical observation for the RBF kernels (Gaussian , Laplace) where the optimal values of the sigma width parameter are shown to lie
in between the 0.1 and 0.9 quantile of the ||xâˆ’x'|| statistics".

```{r}
#install.packages("kernlab")
library(kernlab)
srange = sigest(train_x, frac = 0.5, scaled = FALSE, na.action = na.omit)
srange

```

Fit SVM with Gaussian (rbf) kernel.
```{r}

svm_model=ksvm(train_x,train_y, type = "C-svc", kernel="rbfdot", kpar=list(sigma = srange[["50%"]]), C = 1)

```

Save model on disk.
```{r}
model_filename = "ksvm_model.rds"
saveRDS(svm_model, model_filename)
rm(svm_model)
svm_model2 = readRDS(model_filename)

```
Plot SV.
```{r}
SV = train_x[unlist(alphaindex(svm_model2)),]
plot(SV,pch=19,main="KSVM support vectors", asp=1)


```

Do prediction and show confusion matrix.
```{r}
yp = predict(svm_model2,test_x)
table(test_y,as.numeric( yp>0.5))

```

