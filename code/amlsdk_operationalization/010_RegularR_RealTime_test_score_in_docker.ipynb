{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright (C) Microsoft Corporation.  \n",
    "  \n",
    "## Develop scoring script in a docker container\n",
    "Purpose: Before operationalization (o16n), we show how to develop and test the containerized scripts (o16n python script that invokes the user provided R scoring script) using AML SDK experimentation framework.  \n",
    "  \n",
    "#### Authors\n",
    "\n",
    "* **George Iordanescu** [Microsoft AI CAT](https://github.com/Microsoft/AMLSDKRModelsOperationalization) *Initial work*  \n",
    "* See also the list of [contributors](https://github.com/Microsoft/AMLSDKRModelsOperationalization) who participated in this project.  \n",
    "  \n",
    "Here we use the experimentation (__e13n__) infrastructure in AML SDK to build a docker image that tests score.py script. This docker image alows running R code from python and is the closest proxy to the operationalization (__o16n__) docker image one will get in the next notebooks, where we operationalize the R model. The created docker image is __not__ identical to the o16n image because AML SDK does not allow BYOD (bring your own docker) scenario yet.\n",
    "\n",
    "The score.py script is written in python, but it has an R session created via rpy2. The R model is run via four interactions with an R session:\n",
    " - The init() function in score.py passes the R model file name to the R session which then loads the R model.  \n",
    " - The run() function in score.py passes the jsoned data to be scored to the R session with the model loaded above. jsoned data to be scored are then used with a full R scoring script using rpy2.robjects.r().  \n",
    "\n",
    "Main steps:  \n",
    "* Run score.py script (and the real R scoring script) in a docker image.\n",
    "* Create artifacts for deployment:   \n",
    "  - scoring script file in the project folder (variable __score_script_filename__)  \n",
    "  - conda dependency file (adds R and desired packages to the base docker image)\n",
    "\n",
    "* This covers strictly post e13n steps, so it assumes the existence of R model file (saved as an rds file on disk). This will be registered here so that we can use Model.get_model_path() function inside the init() funstion of the scoring script. See [azureml.core.model.Model](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#get-model-path-model-name--version-none---workspace-none-) for details about encapsulating the model path in the 016n docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check core SDK version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import platform\n",
    "import sys, os\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import ComputeTarget, RemoteCompute \n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "experiment_name = 'test_R_scoring_script'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number, os info and current wd\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "platform.platform()\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility functions like project config params\n",
    "\n",
    "def add_path_to_sys_path(path_to_append):\n",
    "    if not (any(path_to_append in paths for paths in sys.path)):\n",
    "        sys.path.append(path_to_append)\n",
    "\n",
    "auxiliary_files_dir = os.path.join(*(['.', 'src']))\n",
    "\n",
    "paths_to_append = [os.path.join(os.getcwd(), auxiliary_files_dir)]\n",
    "[add_path_to_sys_path(crt_path) for crt_path in paths_to_append]\n",
    "\n",
    "import o16n_regular_ML_R_models_utils\n",
    "prj_consts = o16n_regular_ML_R_models_utils.R_models_operationalization_consts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use existing dotenv file (created in previous notebook) to load sensitive info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "dotenv_file_path = os.path.join(*(prj_consts.DOTENV_FILE_PATH))\n",
    "\n",
    "# #show .env file path\n",
    "# dotenv_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define filename and directory variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model_file_name = prj_consts.R_MODEL_FILE_NAME\n",
    "r_model_AML_name = prj_consts.R_MODEL_AML_NAME\n",
    "conda_dependencies_filename = prj_consts.R_MODEL_CONDA_DEPENDENCIES_FILE_NAME\n",
    "score_script_filename = prj_consts.SCORE_SCRIPT_FILE_NAME\n",
    "\n",
    "workspace_config_dir = os.path.join(*(prj_consts.AML_WORKSPACE_CONFIG_DIR))\n",
    "workspace_config_file = prj_consts.AML_WORKSPACE_CONFIG_FILE_NAME\n",
    "# workspace_config_dir\n",
    "\n",
    "experiment_dir = os.path.join(*(prj_consts.AML_EXPERIMENT_DIR))\n",
    "crt_dir = os.path.join(os.getcwd(), os.path.join(*([experiment_dir])))\n",
    "os.makedirs(crt_dir, exist_ok=True)\n",
    "\n",
    "# make sure exp name is within required limits\n",
    "# len(experiment_name)\n",
    "\n",
    "R_artifacts_dir = os.path.join(os.getcwd(), os.path.join(*(prj_consts.R_MODEL_DIR)))\n",
    "print('Will o16n R model from directory {}'.format(R_artifacts_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the AML SDK workspace (ws) created and documented as a json file in previous notebook\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(path=os.path.join(os.getcwd(), \n",
    "                                             os.path.join(*([workspace_config_dir, 'aml_config', workspace_config_file]))))\n",
    "# print(ws.name, ws.resource_group, ws.location, ws.subscription_id[0], sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "We develop here the scoring (operationalization) script that will use a pre-trained model. Operationalized models can be local files or registered models in the Azure. We show here the latter way. We __can__ access registered models even when using the AML SDK experimentation framework.\n",
    "\n",
    "You can add tags and descriptions to your models. Note you do not need to have the r model .rds file in the current directory.  The below call registers that file in the workspace as a model with name defined by __r_model_AML_name__ variable.  \n",
    "  \n",
    "Using tags, you can track useful information such as the name and version of the machine learning library used to train the model. Note that tags must be alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show model exists at the expected location \n",
    "!ls -l {os.path.join(R_artifacts_dir, r_model_file_name)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model_tags = {'language': 'R', 'type': 'TC_kSVM'}\n",
    "if not Model.list(ws, tags=model_tags):\n",
    "    model = Model.register(model_path = os.path.join(R_artifacts_dir, r_model_file_name),\n",
    "                           model_name = r_model_AML_name,\n",
    "                           tags = model_tags,\n",
    "                           description = 'my R model',\n",
    "                           workspace = ws)\n",
    "    \n",
    "    print(model.name, model.description, model.version, model.tags, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the registered models within your workspace and query by tag. Models are versioned. If you call the register_model command many times with same model name, you will get multiple versions of the model with increasing version numbers.   \n",
    "\n",
    "For demo purposes, we choose v1 as the model used for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_r_model = None\n",
    "\n",
    "for m in Model.list(ws, tags={'type': 'TC_kSVM'}):\n",
    "# for m in r_models:\n",
    "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
    "    if ((m.name==r_model_AML_name) and (m.version==1) and (m.description=='my R model')):\n",
    "        best_r_model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_r_model.name, best_r_model.description, best_r_model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create info for the VM compute target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach a remote Linux VM. This is usually used as a remote docker commpute target for experimentation, but we are using it here to test our dockerized score script used for deploymnet. \n",
    "Create a Linux DSVM in Azure. Make sure you use the Ubuntu flavor, NOT CentOS.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target_name = 'ghiordanXRgpuvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv  $dotenv_file_path\n",
    "\n",
    "attach_config = RemoteCompute.attach_configuration(address=os.getenv('COMPUTE_CONTEXT_VM_FQDN'),\n",
    "                                                   ssh_port=os.getenv('COMPUTE_CONTEXT_VM_SSH_PORT'),\n",
    "                                                   username=os.getenv('COMPUTE_CONTEXT_VM_USER_NAME'),\n",
    "                                                   password=os.getenv('COMPUTE_CONTEXT_VM_PWD')\n",
    "                                                   # If using ssh key\n",
    "                                                   #private_key_file=\"path_to_a_file\",\n",
    "                                                   #private_key_passphrase=\"some_key_phrase\"\n",
    "                                                  )\n",
    "attached_dsvm_compute = ComputeTarget.attach(workspace=ws, name=compute_target_name, attach_configuration=attach_config)\n",
    "\n",
    "attached_dsvm_compute.wait_for_completion(show_output=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the compute target exists in the workspace\n",
    "\n",
    "from azureml.core.compute import DsvmCompute\n",
    "\n",
    "for crt_dsvm in DsvmCompute.list(ws):\n",
    "    if (compute_target_name==crt_dsvm.name):    \n",
    "        print(crt_dsvm.name, crt_dsvm.type, crt_dsvm.address)\n",
    "    else:\n",
    "        print(crt_dsvm.name, crt_dsvm.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create scoring script\n",
    "\n",
    "Use `%%writefile` magic to write o16n `score.py` file  that embeds the user-prvided R scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {os.path.join(experiment_dir, score_script_filename)} \n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from azureml.core.model import Model\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import timeit\n",
    "import logging\n",
    "\n",
    "R_MODEL_AML_NAME = 'trained_r_model'\n",
    "\n",
    "\n",
    "def init():\n",
    "    from rpy2.rinterface import R_VERSION_BUILD\n",
    "    print('rpy2 version {};  R version {}'.format(rpy2.__version__, R_VERSION_BUILD))\n",
    "    \n",
    "    print('R model AML name: {}'.format(Model.get_model_path(model_name=R_MODEL_AML_NAME)))\n",
    "    \n",
    "    global model\n",
    "    # note here \"best_model\" is the name of the model registered under the workspace\n",
    "    # this call should return the path to the model.pkl file on the local disk.\n",
    "    model_path = Model.get_model_path(model_name =  R_MODEL_AML_NAME)\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    robjects.globalenv['model_path'] = model_path    \n",
    "    # model_path = robjects.StrVector( 'ksvm_model.rds')\n",
    "    robjects.r('''\n",
    "            format_proc_time <- function(proc_time_diff){\n",
    "                \n",
    "                as.data.frame(t(as.matrix(format(round(proc_time_diff*1000, 2), nsmall = 2))))[, \n",
    "                                                            c('user.self', 'sys.self', 'elapsed')]\n",
    "            }\n",
    "            library(kernlab)\n",
    "            library(jsonlite)\n",
    "            svm_model = readRDS({model_path})\n",
    "            ''')\n",
    "    print('AML o16n init() function: SVM model loaded.')\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(aml_jsoned_data):\n",
    "    logger = logging.getLogger(\"AML_o16n_run_function\")\n",
    "#     print('Entering run() function')\n",
    "    try:\n",
    "        start_time = timeit.default_timer()\n",
    "#         data = json.loads(raw_data)['data']\n",
    "        data = json.loads(aml_jsoned_data)['data']\n",
    "        robjects.globalenv['r_data_to_score'] = data  \n",
    "        python_to_R_time = timeit.default_timer()\n",
    "        r_messages = robjects.r('''\n",
    "                start_time_r = proc.time()\n",
    "                \n",
    "                r_data_to_score=jsonlite::fromJSON(r_data_to_score[[1]])\n",
    "                json_to_df_time_r = proc.time()\n",
    "                \n",
    "                scores = kernlab::predict(svm_model,r_data_to_score, type = \"p\")\n",
    "                end_time_r = proc.time()\n",
    "                \n",
    "                # report total time and json to df time\n",
    "                time_df = rbind(format_proc_time(end_time_r - start_time_r),\n",
    "                                format_proc_time(json_to_df_time_r - start_time_r))\n",
    "                rownames(time_df)=c('all_r_time','json_to_df_time')    \n",
    "                \n",
    "                # combine scores and time dataframes in a list\n",
    "                returned_list = list(as.data.frame(scores),time_df)\n",
    "                names(returned_list)=c('r_scores', 'r_times')\n",
    "                \n",
    "                scores = jsonlite::toJSON(returned_list)\n",
    "                #print('Exiting R.')\n",
    "                ''')\n",
    "        before_R_to_python_time = timeit.default_timer()\n",
    "        \n",
    "        jsoned_scores = (robjects.r['scores'])[0]\n",
    "        end_time = timeit.default_timer()\n",
    "        \n",
    "#         logger.info(\"Predictions: {0}\".format(jsoned_scores))\n",
    "#         print('Exiting run() function')\n",
    "        return json.dumps({'python_scores': jsoned_scores, \n",
    "                           'python_times': json.dumps(\n",
    "                               {'all_p_time':'{} ms'.format(round((end_time-start_time)*1000, 2)),\n",
    "                                           'python_to_R_time':'{} ms'.format(round((python_to_R_time-start_time)*1000, 2)),\n",
    "                                           'R_to_python_time':'{} ms'.format(round((end_time-before_R_to_python_time)*1000, 2))}\n",
    "                           )\n",
    "                          })\n",
    "\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"AML o16n run() function: error\": result})\n",
    "    \n",
    "def main():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    n_samples = 100\n",
    "\n",
    "    raw_data = 2 * np.random.random_sample((n_samples, 2)) - 1\n",
    "    aml_jsoned_data =  json.dumps({'data': json.dumps(raw_data.tolist())})\n",
    "  \n",
    "    init()\n",
    "    response = run(aml_jsoned_data)\n",
    "#     print(json.loads(response))\n",
    "#     print( json.loads(json.loads(response)['python_scores']) )\n",
    "    \n",
    "    print( pd.DataFrame.from_records(json.loads(json.loads(response)['python_scores'])['r_scores']) )\n",
    "    print( pd.DataFrame.from_records(json.loads(json.loads(response)['python_scores'])['r_times']) )\n",
    "    for k, v in json.loads(json.loads(response)['python_times']).items():\n",
    "        print(v, k)\n",
    "\n",
    "    print('Exited main() function')\n",
    "    \n",
    "if __name__== \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a Docker run with new conda environment on the VM  \n",
    "You can execute in a Docker container in the VM. If you choose this route, you don't need to install anything on the VM yourself. Azure ML execution service will take care of it for you. You can also build a custom Docker image, and execute script in it without building a new conda environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a run using a custom Docker image & user-managed environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration(framework = \"python\")\n",
    "run_config.target = attached_dsvm_compute.name\n",
    "\n",
    "# Use Docker in the remote VM\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "run_config.environment.docker.base_image = 'continuumio/miniconda3:4.5.12'\n",
    "\n",
    "# Ask system to provision a new one based on the conda_dependencies.yml file\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "run_config.environment.docker.gpu_support = False\n",
    "run_config.environment.docker.shared_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the crux of the R models o16n project:\n",
    "> Instead of install.packages() using a live R session, we use a conda env .yml file to conda and pip install R and R (and python) packages to a conda environment that will run in our docker container!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the conda env yml file\n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "def add_conda_items(list_of_items, item_add_method):\n",
    "    for current_item in list_of_items:\n",
    "        item_add_method(current_item)\n",
    "    \n",
    "add_conda_items(['3.7.0'], getattr(conda_dep, 'set_python_version')) #'3.6.5' \n",
    "add_conda_items(['r', 'conda-forge', 'anaconda'], getattr(conda_dep, 'add_channel'))\n",
    "add_conda_items(['r-base', 'r-proc', 'r-jsonlite', 'r-kernlab', 'rpy2', 'pandas', 'gfortran_linux-64'], \n",
    "                getattr(conda_dep, 'add_conda_package')) #'rpy2==2.8.6'\n",
    "# add_conda_items(['some_pip_installable_R_package'], getattr(conda_dep, 'add_pip_package'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For demo purposes we also show how a generic (i.e. not SDK created) conda env file can be used in SDK\n",
    "\n",
    "First show the content of the conda dep object above, so that we can create a clone of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda_dep.serialize_to_string()\n",
    "conda_dep.save_to_file(base_directory = os.getcwd() , \n",
    "                       conda_file_path=os.path.join(*[experiment_dir, conda_dependencies_filename]))\n",
    "! cat {os.path.join(os.getcwd(), os.path.join(*[experiment_dir, conda_dependencies_filename]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_SDK_created_conda_env_file = 'not_SDK_created_conda_env_file.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./{not_SDK_created_conda_env_file}\n",
    "\n",
    "name: ml_conda_env2\n",
    "\n",
    "channels:\n",
    "- r\n",
    "- conda-forge\n",
    "- anaconda\n",
    "\n",
    "dependencies:\n",
    "- python=3.7.0\n",
    "- r-base\n",
    "- r-proc\n",
    "- r-jsonlite\n",
    "- r-kernlab\n",
    "- rpy2\n",
    "- pandas\n",
    "- gfortran_linux-64\n",
    "- pip:\n",
    "    # Required packages for AzureML execution, history, and data preparation.\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can use either of the commands below to set the conda dependencies either from a yml file, or from the CondaDependencies conda_dep object created in memory above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_config.environment.python.conda_dependencies = CondaDependencies(not_SDK_created_conda_env_file)\n",
    "run_config.environment.python.conda_dependencies = conda_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Experiment\n",
    "Submit script to run in the Docker image in the remote VM. If you run this for the first time, the system will download the base image, layer in packages specified in the `conda_dependencies.yml` file on top of the base image, create a container and then execute the script in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(source_directory = experiment_dir, script = score_script_filename, run_config = run_config)\n",
    "run = exp.submit(src)\n",
    "\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_portal_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter widget\n",
    "Watch the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get log results upon completion\n",
    "Scoring script runs in the background. You can use wait_for_completion to block and wait until all data is cored before running more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to recover a previous run\n",
    "# run = Run(exp, 'runID')\n",
    "\n",
    "# to get more details\n",
    "run.get_details_with_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now ready for deployment. In the next notebook we will package the scoring script in an o16n image and deploy it as web service on an Azure Container Instance and an AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html 010_RegularR_RealTime_test_score_in_docker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
